{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RSNA Intracranial Hemorrhage Detection "},{"metadata":{},"cell_type":"markdown","source":"<b>What am i predicting?</b><br/><br/>\nIn this competition our goal is to predict intracranial hemorrhage and its subtypes. Given an image the we need to predict probablity of each subtype. This indicates its a multilabel classification problem."},{"metadata":{},"cell_type":"markdown","source":"<b>Competition Evaluation Metric</b><br/><br/>\nEvaluation metric is weighted multi-label logarithmic loss. So for given image we need to predict probality for each subtype. There is also an any label, which indicates that a hemorrhage of ANY kind exists in the image. The any label is weighted more highly than specific hemorrhage sub-types.\n\n<b>Note:</b>The weights for each subtype for calculating weighted multi-label logarithmic loss is **not** given as part of the competition. We will be using binary cross entropy loss as weights are not available"},{"metadata":{},"cell_type":"markdown","source":"<b>Dataset Description</b>\n\nThe dataset is divided into two parts\n\n1. Train\n2. Test\n\n**1. Train**\nNumber of rows: 40,45,548 records.\nNumber of columns: 2\n\nColumns:\n\n**Id**: An image Id. Each Id corresponds to a unique image, and will contain an underscore.\n\nExample: ID_28fbab7eb_epidural. So the Id consists of two parts one is image file id ID_28fbab7eb and the other is sub type name\n\n**Label**: The target label whether that sub-type of hemorrhage (or any hemorrhage in the case of any) exists in the indicated image. 1 --> Exists and 0 --> Doesn't exist.\n\n**2. Test**\nNumber of rows: 4,71,270 records.\n\nColumns:\n\n**Id**: An image Id. Each Id corresponds to a unique image, and will contain an underscore.\n\nExample: ID_28fbab7eb_epidural. So the Id consists of two parts one is image file id ID_28fbab7eb and the other is sub type name"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport glob\nimport random\nimport cv2\nimport tensorflow as tf\nfrom math import ceil, floor\nfrom tqdm import tqdm\nfrom imgaug import augmenters as iaa\nimport matplotlib.pyplot as plt\nfrom math import ceil, floor\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.models import Model, load_model\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.utils import Sequence\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Seed\nSEED = 42\nnp.random.seed(SEED)\n\n# some constants\nTEST_SIZE = 0.06\nHEIGHT = 224\nWIDTH = 224\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\n\n# Train and Test folders\ninput_folder = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\npath_train_img = input_folder + 'stage_2_train/'\npath_test_img = input_folder + 'stage_2_test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(input_folder + 'stage_2_train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract subtype\ntrain_df['sub_type'] = train_df['ID'].apply(lambda x: x.split('_')[-1])\n# extract filename\ntrain_df['file_name'] = train_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove duplicates\ntrain_df.drop_duplicates(['Label', 'sub_type', 'file_name'], inplace=True)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of train images availabe:\", len(os.listdir(path_train_img)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df = pd.pivot_table(train_df.drop(columns='ID'), index=\"file_name\", \\\n                                columns=\"sub_type\", values=\"Label\")\ntrain_final_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Invalid image ID_6431af929.dcm\ntrain_final_df.drop('ID_6431af929.dcm', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_small_df = train_final_df.head(0)\n\nepidural_df = train_final_df[train_final_df.epidural == 1]\nintraparenchymal_df = train_final_df[train_final_df.intraparenchymal == 1]\nintraventricular_df = train_final_df[train_final_df.intraventricular == 1]\nsubarachnoid_df = train_final_df[train_final_df.subarachnoid == 1]\nsubdural_df = train_final_df[train_final_df.subdural == 1]\n\nnon_df=train_final_df[(train_final_df.epidural == 0) & (train_final_df.intraparenchymal == 0) & \\\n                      (train_final_df.intraventricular == 0) & (train_final_df.subarachnoid == 0) & \\\n                      (train_final_df.subdural == 0)]\n\ntrain_small_df = pd.concat([train_small_df,epidural_df[:2000],intraparenchymal_df[:2000],\\\n                            intraventricular_df[:2000],subarachnoid_df[:2000],subdural_df[:2000],\\\n                            non_df[:10000] ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dicom_field_value(val):\n    if type(val) == pydicom.multival.MultiValue:\n        return int(val[0])\n    else:\n        return int(val)\n\ndef get_windowing(data):\n    dicom_fields = [data.WindowCenter, data.WindowWidth, \\\n                    data.RescaleSlope, data.RescaleIntercept]\n    return [get_dicom_field_value(x) for x in dicom_fields]\n\ndef get_windowed_image(image, wc, ww, slope, intercept):\n    img = (image*slope +intercept)\n    img_min = wc - ww//2\n    img_max = wc + ww//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img \n\n\ndef _normalize(img):\n    if img.max() == img.min():\n        return np.zeros(img.shape)\n    return 2 * (img - img.min())/(img.max() - img.min()) - 1\n\ndef _read(path, desired_size=(224, 224)):\n    # 1. read dicom file\n    dcm = pydicom.dcmread(path)\n    \n    # 2. Extract meta data features\n    # window center, window width, slope, intercept\n    window_params = get_windowing(dcm)\n\n    try:\n        # 3. Generate windowed image\n        img = get_windowed_image(dcm.pixel_array, *window_params)\n    except:\n        img = np.zeros(desired_size)\n\n    img = _normalize(img)\n\n    if desired_size != (512, 512):\n        # resize image\n        img = cv2.resize(img, desired_size, interpolation = cv2.INTER_LINEAR)\n    return img[:,:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_read(path_train_img + 'ID_ffff922b9.dcm', (128, 128)).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(\n    _read(path_train_img + 'ID_ffff922b9.dcm', (128, 128))[:, :, 0]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Data Generator\nclass TrainDataGenerator(keras.utils.Sequence):\n\n    def __init__(self, dataset, labels, batch_size=16, img_size=(512, 512), img_dir = path_train_img, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X, Y = self.__data_generation(indices)\n        return X, Y\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n        np.random.shuffle(self.indices)\n        \n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size, 3))\n        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir + ID, self.img_size)\n            X[i,] = image            \n            Y[i,] = self.labels.iloc[index].values        \n        return X, Y\n    \nclass TestDataGenerator(keras.utils.Sequence):\n    def __init__(self, ids, labels, batch_size = 5, img_size = (512, 512), img_dir = path_test_img, \\\n                 *args, **kwargs):\n        self.ids = ids\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.ids[k] for k in indices]\n        X = self.__data_generation(list_IDs_temp)\n        return X\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n\n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, *self.img_size, 3))\n        for i, ID in enumerate(list_IDs_temp):\n            image = _read(self.img_dir + ID, self.img_size)\n            X[i,] = image            \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen in EDA notebook that we have very few epidural subtypes so we need oversample this sub type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling\n#epidural_df = train_final_df[train_final_df.epidural == 1]\n#train_final_df = pd.concat([train_final_df, epidural_df])\n#print('Train Shape: {}'.format(train_final_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load test set\ntest_df = pd.read_csv(input_folder + 'stage_2_sample_submission.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract subtype\ntest_df['sub_type'] = test_df['ID'].apply(lambda x: x.split('_')[-1])\n# extract filename\ntest_df['file_name'] = test_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\n\ntest_df = pd.pivot_table(test_df.drop(columns='ID'), index=\"file_name\", \\\n                                columns=\"sub_type\", values=\"Label\")\ntest_df.head()\n\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"base_model =  ResNet50(weights = 'imagenet', include_top = False, \\\n                                 pooling = 'avg', input_shape = (HEIGHT, WIDTH, 3))\nx = base_model.output\n#x = Dropout(0.125)(x)\noutput_layer = Dense(6, activation = 'sigmoid')(x)\nmodel = Model(inputs=base_model.input, outputs=output_layer)\nmodel.compile(optimizer = Adam(learning_rate = 0.0001), \n                  loss = 'binary_crossentropy',\n                  metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/trent-b/iterative-stratification\n# Mutlilabel stratification\nsplits = StratifiedShuffleSplit(n_splits = 2, test_size = TEST_SIZE, random_state = SEED)\nfile_names = train_small_df.index\nlabels = train_small_df.values\n# Lets take only the first split\nsplit = next(splits.split(file_names, labels))\ntrain_idx = split[0]\nvalid_idx = split[1]\nsubmission_predictions = []\nlen(train_idx), len(valid_idx)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data generator\ndata_generator_train = TrainDataGenerator(train_small_df.iloc[train_idx], \n                                                train_small_df.iloc[train_idx], \n                                                TRAIN_BATCH_SIZE, \n                                                (WIDTH, HEIGHT))\n\n# validation data generator\ndata_generator_val = TrainDataGenerator(train_small_df.iloc[valid_idx], \n                                            train_small_df.iloc[valid_idx], \n                                            VALID_BATCH_SIZE, \n                                            (WIDTH, HEIGHT))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data_generator_train), len(data_generator_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Competition evaluation metric is evaluated based on weighted log loss but we haven't given weights for each subtype but as per discussion from this thread https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109526#latest-630190 any has a wieght of 2 than other types below sample is taken from the discussion threas"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a single epoch we are going to train only last 5 layers of Efficient. Since we have a large number of images around 600k so its better to train the all the layers on the whole train dataset but due its high computation resources required to train we only goin to train last five layers on whole dataset and for rest of epochs we only train on a sample of dataset but will train all the layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(generator = data_generator_train,\n                            validation_data = data_generator_val,\n                            epochs = 5,\n                            verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install gdown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!gdown https://drive.google.com/uc?id=1kZmMCCBOWSjCZjz2XWaouDIj5gFn2D-q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!cp \"model (4).h5\" model.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.empty((1, 224, 224, 3))\nX[0, ] = _read(path_train_img + 'ID_000012eaf.dcm', (224, 224))\nplt.imshow(\n    _read(path_train_img + 'ID_000012eaf.dcm', (224, 224))[:, :, 0]\n )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.argmax(model.predict(X))\n\ny = model.predict(X)\npred = zip(y.tolist()[0], train_small_df.columns)\nfor i in pred:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(train_small_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have preditions for each of the image\n# We need to make 6 rows for each of file according to the subtype\nids = []\nvalues = []\nfor i, j in tqdm(zip(pred, test_df.index.to_list())):\n#     print(i, j)\n    # i=[any_prob, epidural_prob, intraparenchymal_prob, intraventricular_prob, subarachnoid_prob, subdural_prob]\n    # j = filename ==> ID_xyz.dcm\n    for k in range(i.shape[0]):\n        ids.append([j.replace('.dcm', '_' + cols[k])])\n        values.append(i[k])      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=ids)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv(input_folder + 'stage_2_sample_submission.csv')\nsample_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Label'] = values\ndf.columns = sample_df.columns\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_download_link(filename='submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}