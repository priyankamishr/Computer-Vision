{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Intracranial Hemorrhage Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What am i predicting?</b><br/><br/>\n",
    "In this competition our goal is to predict intracranial hemorrhage and its subtypes. Given an image the we need to predict probablity of each subtype. This indicates its a multilabel classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Competition Evaluation Metric</b><br/><br/>\n",
    "Evaluation metric is weighted multi-label logarithmic loss. So for given image we need to predict probality for each subtype. There is also an any label, which indicates that a hemorrhage of ANY kind exists in the image. The any label is weighted more highly than specific hemorrhage sub-types.\n",
    "\n",
    "<b>Note:</b>The weights for each subtype for calculating weighted multi-label logarithmic loss is **not** given as part of the competition. We will be using binary cross entropy loss as weights are not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset Description</b>\n",
    "\n",
    "The dataset is divided into two parts\n",
    "\n",
    "1. Train\n",
    "2. Test\n",
    "\n",
    "**1. Train**\n",
    "Number of rows: 40,45,548 records.\n",
    "Number of columns: 2\n",
    "\n",
    "Columns:\n",
    "\n",
    "**Id**: An image Id. Each Id corresponds to a unique image, and will contain an underscore.\n",
    "\n",
    "Example: ID_28fbab7eb_epidural. So the Id consists of two parts one is image file id ID_28fbab7eb and the other is sub type name\n",
    "\n",
    "**Label**: The target label whether that sub-type of hemorrhage (or any hemorrhage in the case of any) exists in the indicated image. 1 --> Exists and 0 --> Doesn't exist.\n",
    "\n",
    "**2. Test**\n",
    "Number of rows: 4,71,270 records.\n",
    "\n",
    "Columns:\n",
    "\n",
    "**Id**: An image Id. Each Id corresponds to a unique image, and will contain an underscore.\n",
    "\n",
    "Example: ID_28fbab7eb_epidural. So the Id consists of two parts one is image file id ID_28fbab7eb and the other is sub type name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from math import ceil, floor\n",
    "from tqdm import tqdm\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, floor\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.utils import Sequence\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# some constants\n",
    "TEST_SIZE = 0.06\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "\n",
    "# Train and Test folders\n",
    "input_folder = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\n",
    "path_train_img = input_folder + 'stage_2_train/'\n",
    "path_test_img = input_folder + 'stage_2_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(input_folder + 'stage_2_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subtype\n",
    "train_df['sub_type'] = train_df['ID'].apply(lambda x: x.split('_')[-1])\n",
    "# extract filename\n",
    "train_df['file_name'] = train_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "train_df.drop_duplicates(['Label', 'sub_type', 'file_name'], inplace=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train images availabe:\", len(os.listdir(path_train_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = pd.pivot_table(train_df.drop(columns='ID'), index=\"file_name\", \\\n",
    "                                columns=\"sub_type\", values=\"Label\")\n",
    "train_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid image ID_6431af929.dcm\n",
    "train_final_df.drop('ID_6431af929.dcm', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df = train_final_df.head(0)\n",
    "\n",
    "epidural_df = train_final_df[train_final_df.epidural == 1]\n",
    "intraparenchymal_df = train_final_df[train_final_df.intraparenchymal == 1]\n",
    "intraventricular_df = train_final_df[train_final_df.intraventricular == 1]\n",
    "subarachnoid_df = train_final_df[train_final_df.subarachnoid == 1]\n",
    "subdural_df = train_final_df[train_final_df.subdural == 1]\n",
    "\n",
    "non_df=train_final_df[(train_final_df.epidural == 0) & (train_final_df.intraparenchymal == 0) & \\\n",
    "                      (train_final_df.intraventricular == 0) & (train_final_df.subarachnoid == 0) & \\\n",
    "                      (train_final_df.subdural == 0)]\n",
    "\n",
    "train_small_df = pd.concat([train_small_df,epidural_df[:2000],intraparenchymal_df[:2000],\\\n",
    "                            intraventricular_df[:2000],subarachnoid_df[:2000],subdural_df[:2000],\\\n",
    "                            non_df[:10000] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_field_value(val):\n",
    "    if type(val) == pydicom.multival.MultiValue:\n",
    "        return int(val[0])\n",
    "    else:\n",
    "        return int(val)\n",
    "\n",
    "def get_windowing(data):\n",
    "    dicom_fields = [data.WindowCenter, data.WindowWidth, \\\n",
    "                    data.RescaleSlope, data.RescaleIntercept]\n",
    "    return [get_dicom_field_value(x) for x in dicom_fields]\n",
    "\n",
    "def get_windowed_image(image, wc, ww, slope, intercept):\n",
    "    img = (image*slope +intercept)\n",
    "    img_min = wc - ww//2\n",
    "    img_max = wc + ww//2\n",
    "    img[img<img_min] = img_min\n",
    "    img[img>img_max] = img_max\n",
    "    return img \n",
    "\n",
    "\n",
    "def _normalize(img):\n",
    "    if img.max() == img.min():\n",
    "        return np.zeros(img.shape)\n",
    "    return 2 * (img - img.min())/(img.max() - img.min()) - 1\n",
    "\n",
    "def _read(path, desired_size=(224, 224)):\n",
    "    # 1. read dicom file\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    \n",
    "    # 2. Extract meta data features\n",
    "    # window center, window width, slope, intercept\n",
    "    window_params = get_windowing(dcm)\n",
    "\n",
    "    try:\n",
    "        # 3. Generate windowed image\n",
    "        img = get_windowed_image(dcm.pixel_array, *window_params)\n",
    "    except:\n",
    "        img = np.zeros(desired_size)\n",
    "\n",
    "    img = _normalize(img)\n",
    "\n",
    "    if desired_size != (512, 512):\n",
    "        # resize image\n",
    "        img = cv2.resize(img, desired_size, interpolation = cv2.INTER_LINEAR)\n",
    "    return img[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_read(path_train_img + 'ID_ffff922b9.dcm', (128, 128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    _read(path_train_img + 'ID_ffff922b9.dcm', (128, 128))[:, :, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data Generator\n",
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, dataset, labels, batch_size=16, img_size=(512, 512), img_dir = path_train_img, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.ids = dataset.index\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, Y = self.__data_generation(indices)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size, 3))\n",
    "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            ID = self.ids[index]\n",
    "            image = _read(self.img_dir + ID, self.img_size)\n",
    "            X[i,] = image            \n",
    "            Y[i,] = self.labels.iloc[index].values        \n",
    "        return X, Y\n",
    "    \n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ids, labels, batch_size = 5, img_size = (512, 512), img_dir = path_test_img, \\\n",
    "                 *args, **kwargs):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.ids[k] for k in indices]\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size, 3))\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            image = _read(self.img_dir + ID, self.img_size)\n",
    "            X[i,] = image            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in EDA notebook that we have very few epidural subtypes so we need oversample this sub type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "#epidural_df = train_final_df[train_final_df.epidural == 1]\n",
    "#train_final_df = pd.concat([train_final_df, epidural_df])\n",
    "#print('Train Shape: {}'.format(train_final_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_df = pd.read_csv(input_folder + 'stage_2_sample_submission.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subtype\n",
    "test_df['sub_type'] = test_df['ID'].apply(lambda x: x.split('_')[-1])\n",
    "# extract filename\n",
    "test_df['file_name'] = test_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\n",
    "\n",
    "test_df = pd.pivot_table(test_df.drop(columns='ID'), index=\"file_name\", \\\n",
    "                                columns=\"sub_type\", values=\"Label\")\n",
    "test_df.head()\n",
    "\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =  ResNet50(weights = 'imagenet', include_top = False, \\\n",
    "                                 pooling = 'avg', input_shape = (HEIGHT, WIDTH, 3))\n",
    "x = base_model.output\n",
    "#x = Dropout(0.125)(x)\n",
    "output_layer = Dense(6, activation = 'sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), \n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/trent-b/iterative-stratification\n",
    "# Mutlilabel stratification\n",
    "splits = StratifiedShuffleSplit(n_splits = 2, test_size = TEST_SIZE, random_state = SEED)\n",
    "file_names = train_small_df.index\n",
    "labels = train_small_df.values\n",
    "# Lets take only the first split\n",
    "split = next(splits.split(file_names, labels))\n",
    "train_idx = split[0]\n",
    "valid_idx = split[1]\n",
    "submission_predictions = []\n",
    "len(train_idx), len(valid_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data generator\n",
    "data_generator_train = TrainDataGenerator(train_small_df.iloc[train_idx], \n",
    "                                                train_small_df.iloc[train_idx], \n",
    "                                                TRAIN_BATCH_SIZE, \n",
    "                                                (WIDTH, HEIGHT))\n",
    "\n",
    "# validation data generator\n",
    "data_generator_val = TrainDataGenerator(train_small_df.iloc[valid_idx], \n",
    "                                            train_small_df.iloc[valid_idx], \n",
    "                                            VALID_BATCH_SIZE, \n",
    "                                            (WIDTH, HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_generator_train), len(data_generator_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition evaluation metric is evaluated based on weighted log loss but we haven't given weights for each subtype but as per discussion from this thread https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109526#latest-630190 any has a wieght of 2 than other types below sample is taken from the discussion threas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single epoch we are going to train only last 5 layers of Efficient. Since we have a large number of images around 600k so its better to train the all the layers on the whole train dataset but due its high computation resources required to train we only goin to train last five layers on whole dataset and for rest of epochs we only train on a sample of dataset but will train all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator = data_generator_train,\n",
    "                            validation_data = data_generator_val,\n",
    "                            epochs = 5,\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gdown https://drive.google.com/uc?id=1kZmMCCBOWSjCZjz2XWaouDIj5gFn2D-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp \"model (4).h5\" model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((1, 224, 224, 3))\n",
    "X[0, ] = _read(path_train_img + 'ID_000012eaf.dcm', (224, 224))\n",
    "plt.imshow(\n",
    "    _read(path_train_img + 'ID_000012eaf.dcm', (224, 224))[:, :, 0]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(model.predict(X))\n",
    "\n",
    "y = model.predict(X)\n",
    "pred = zip(y.tolist()[0], train_small_df.columns)\n",
    "for i in pred:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_small_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have preditions for each of the image\n",
    "# We need to make 6 rows for each of file according to the subtype\n",
    "ids = []\n",
    "values = []\n",
    "for i, j in tqdm(zip(pred, test_df.index.to_list())):\n",
    "#     print(i, j)\n",
    "    # i=[any_prob, epidural_prob, intraparenchymal_prob, intraventricular_prob, subarachnoid_prob, subdural_prob]\n",
    "    # j = filename ==> ID_xyz.dcm\n",
    "    for k in range(i.shape[0]):\n",
    "        ids.append([j.replace('.dcm', '_' + cols[k])])\n",
    "        values.append(i[k])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=ids)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(input_folder + 'stage_2_sample_submission.csv')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = values\n",
    "df.columns = sample_df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_download_link(filename='submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
